{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>ADMN5015 Artificial Intelligence in Marketing</h1>\n",
    "<h2>Assignment 2: Classification using Tensorflow\n",
    "<h3>Katrina Ong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "This project aims to classify customers who are likely to churn or not likely to churn for a tour & travel company. The dataset was obtained through [Kaggle](https://www.kaggle.com/datasets/tejashvi14/tour-travels-customer-churn-prediction). \n",
    "\n",
    "To build a Tensorflow classification model, the following steps are implemented in this notebook: \n",
    "1. Import Packages\n",
    "2. Source the Data\n",
    "3. Explore the Data\n",
    "4. Prepare the Data (Feature Engineering)\n",
    "5. Build and Test Tensorflow Model\n",
    "6. Evaluate Tensorflow Model\n",
    "7. Predict on New Cases\n",
    "\n",
    "More details can be found in each line of code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of Customer Churn Prediction**\n",
    "\n",
    "Being able to classify customers who will churn vs. customers who will not churn allows a company to take proactive approaches in retaining customers. By know which customers will likely churn, the company can target these customers, understand their behavior, and tailor their marketing strategy accordingly.\n",
    "\n",
    "**Communication to Management**\n",
    "\n",
    "Effective communication to management is key in adopting machine learning methods and analytics in an organization's marketing strategy. Benefits may be communicated in terms of the value of the business that could be saved if this model is adopted. For example, the value of business lost due to customers churning can be highlighted in comparison to the prediction accuracy of the model. \n",
    "\n",
    "The potential savings would amount to the following:\n",
    "\n",
    "``` Potential Savings = (Value of Business Lost x Recall Rate) - Amount of Proactive Marketing Measures Undertaken ```\n",
    "\n",
    "*Note: Recall Rate is used since it denotes the percentage of true positives (i.e., customers who will churn) detected*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Tensorflow model was able to achieve the following performance metrics:\n",
    "- Accuracy: 0.8348 \n",
    "- Recall: 0.8039 \n",
    "- Area Under the Curve (AUC): 0.8694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:** [[1]](https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed), [[2]](https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/), [[3]](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Standard packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import random\n",
    "import os\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "from time import strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44368\\1886167303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "# Feature Engineering Packages\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "#from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced learning library\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import (\n",
    "    SMOTE,\n",
    "    BorderlineSMOTE,\n",
    "    SVMSMOTE,\n",
    ")\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Options for display\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale = 2)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random state\n",
    "SEED = 17\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random state for tensorflow\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Source]](https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Source the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source data from CSV file\n",
    "df = pd.read_csv('Customertravel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few records\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview last few records\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes per column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary statistics of numeric variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and check the value counts/classes of the target variable\n",
    "# 1 - Customer Churn\n",
    "# 0 - Customer does not churn\n",
    "\n",
    "df[\"Target\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count of target class\n",
    "plt.rcParams['font.size'] = 15\n",
    "ax = sns.countplot(data = df, x = \"Target\")\n",
    "abs_values = df['Target'].value_counts(ascending=False).values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values); #imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Correlation Heatmap\n",
    "sns.heatmap(df.corr(),annot = True, annot_kws={\"fontsize\":18}, cmap='OrRd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of categorical variables\n",
    "cat_cols = ['FrequentFlyer','AnnualIncomeClass','AccountSyncedToSocialMedia','BookedHotelOrNot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the counts of values for each categorical variable \n",
    "fig,axes = plt.subplots(2,2,figsize=(30,20))\n",
    "plt.rcParams['font.size'] = 25\n",
    "\n",
    "for idx,cat_col in enumerate(cat_cols):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(data=df,x=cat_col,ax=axes[row,col])\n",
    "    abs_values = df[cat_col].value_counts(ascending=False).values\n",
    "    axes[row,col].bar_label(container=axes[row,col].containers[0], labels=abs_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a) One-Hot Encoding of Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for one-hot encodring\n",
    "onehot_cols = cat_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Annual Income Class since it is an ordinal column\n",
    "onehot_cols.remove(\"AnnualIncomeClass\")\n",
    "onehot_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate One-Hot encoding\n",
    "onehot_encoder = ce.one_hot.OneHotEncoder(cols=onehot_cols,use_cat_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit-Transform to Dataset\n",
    "df = onehot_encoder.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b) Ordinal Encoding for Ordered Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of categories for ordinal encoding - lower number for lower rank\n",
    "d_incomeClass = {'col': 'AnnualIncomeClass', 'mapping': {'Low Income': 0, 'Middle Income': 1, 'High Income': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Ordinal Encoding\n",
    "ordinal_encoder = ce.ordinal.OrdinalEncoder(cols='AnnualIncomeClass',mapping = [d_incomeClass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit-Transform to Dataset\n",
    "df = ordinal_encoder.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview Data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c) Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "df = df.drop(df[df['FrequentFlyer_No Record'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if records with missing values have been removed\n",
    "df['FrequentFlyer_No Record'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d) Delete Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop No Record encoded column\n",
    "df = df.drop('FrequentFlyer_No Record',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking datatypes and names of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns to delete\n",
    "cols_to_delete = ['FrequentFlyer_No','AccountSyncedToSocialMedia_No','BookedHotelOrNot_No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns \n",
    "df = df.drop(cols_to_delete,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if information has been deleted\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3e) Visualize Cleaned Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count of target class\n",
    "plt.rcParams['font.size'] = 15\n",
    "ax = sns.countplot(data = df, x = \"Target\")\n",
    "abs_values = df['Target'].value_counts(ascending=False).values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values); #imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list of categorical variables\n",
    "cat_cols = ['FrequentFlyer_Yes','AnnualIncomeClass','AccountSyncedToSocialMedia_Yes','BookedHotelOrNot_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the counts of values for each categorical variable \n",
    "fig,axes = plt.subplots(2,2,figsize=(30,20))\n",
    "plt.rcParams['font.size'] = 25\n",
    "\n",
    "for idx,cat_col in enumerate(cat_cols):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(data=df,x=cat_col,ax=axes[row,col])\n",
    "    abs_values = df[cat_col].value_counts(ascending=False).values\n",
    "    axes[row,col].bar_label(container=axes[row,col].containers[0], labels=abs_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3f) Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Features from Target\n",
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3g) Resampling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate SMOTE\n",
    "sm = SMOTE(\n",
    "    sampling_strategy='auto',  \n",
    "    random_state=21,  \n",
    "    k_neighbors=5,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate ENN\n",
    "enn = EditedNearestNeighbours(\n",
    "    sampling_strategy='auto',\n",
    "    n_neighbors=3,\n",
    "    kind_sel='all',\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Rebalancing Methods\n",
    "method = SMOTEENN(\n",
    "    sampling_strategy='auto',  \n",
    "    random_state=21,  \n",
    "    smote=sm,\n",
    "    enn=enn,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply resampling to training data only\n",
    "X_train_rs, y_train_rs = method.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rs.value_counts() #resampling improved the balance between majority class and minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3h) Scale the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Column Transformer to apply Standard Scaler to relevant columns and ignore/passthrough remaning date time columns\n",
    "ct = ColumnTransformer([(\"scaler\", StandardScaler(),['Age','ServicesOpted'])],\n",
    "                        remainder = 'passthrough') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Transform on Training Data\n",
    "X_train_sc = ct.fit_transform(X_train_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform on Test Data\n",
    "X_test_sc = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of the training and testing data\n",
    "X_train_sc.shape, X_test_sc.shape, y_train_rs.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3i) Convert to Tensorflow Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and testing features to Tensorflow Objects \n",
    "X_train_tf = tf.convert_to_tensor(X_train_sc)\n",
    "X_test_tf = tf.convert_to_tensor(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build and Test Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "def get_basic_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    #normalizer,\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    #tf.keras.layers.Dense(2, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Possible labels 0 or 1\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #if output is more than 2\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), #if output is 2, uncomment this\n",
    "                metrics=['Accuracy','Recall','AUC'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining callback to signal the model to stop learning\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0.0001,\n",
    "    patience=30,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "BATCH_SIZE = 2**8 #has to be in power of 2\n",
    "\n",
    "model = get_basic_model()\n",
    "\n",
    "model.fit(X_train_tf, y_train_rs, epochs=4000, batch_size=BATCH_SIZE, callbacks = [callback]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Evaluate Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test data based on training data\n",
    "\n",
    "score = model.evaluate(X_test_tf, y_test, verbose=1)\n",
    "\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]} / Test Recall: {score[2]} / Test AUC: {score[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('travelchurn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Predict on New Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of columns\n",
    "ind_features = list(df.columns.values).remove('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for of 3 new cases\n",
    "df_predict = pd.DataFrame(columns = ind_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new case\n",
    "sample1 = {'Age': 34,\n",
    "           'FrequentFlyer_Yes': 1,\n",
    "           'AnnualIncomeClass': 0,\n",
    "           'ServicesOpted':5,\n",
    "           'AccountSyncedToSocialMedia_Yes': 1,\n",
    "           'BookedHotelOrNot_Yes':0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new case\n",
    "sample2 = {'Age': 45,\n",
    "           'FrequentFlyer_Yes': 1,\n",
    "           'AnnualIncomeClass': 2,\n",
    "           'ServicesOpted':5,\n",
    "           'AccountSyncedToSocialMedia_Yes': 1,\n",
    "           'BookedHotelOrNot_Yes':1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new case\n",
    "sample3 = {'Age': 60,\n",
    "           'FrequentFlyer_Yes': 1,\n",
    "           'AnnualIncomeClass': 2,\n",
    "           'ServicesOpted':2,\n",
    "           'AccountSyncedToSocialMedia_Yes': 0,\n",
    "           'BookedHotelOrNot_Yes':1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new samples to dataframe\n",
    "df_predict = df_predict.append(sample1, ignore_index=True)\n",
    "df_predict = df_predict.append(sample2, ignore_index=True)\n",
    "df_predict = df_predict.append(sample3, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview Dataframe\n",
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of columns\n",
    "col_list = list(df_predict.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data\n",
    "df_predict = ct.fit_transform(df_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data back into a dataframe\n",
    "df_predict = pd.DataFrame(df_predict, columns = col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to Tensorflow tensor\n",
    "\n",
    "predict_numeric_features = tf.convert_to_tensor(df_predict)\n",
    "\n",
    "predict_numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels\n",
    "\n",
    "class_names = ['Does not Churn', 'Churn']\n",
    "\n",
    "predictions = model(predict_numeric_features, training=False)\n",
    "\n",
    "# Create new columns in dataframe\n",
    "df_predict['label'] = None\n",
    "df_predict['certainty'] = None\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "  class_idx = tf.argmax(logits).numpy()\n",
    "  p = tf.nn.softmax(logits)[class_idx]\n",
    "  name = class_names[class_idx]\n",
    "  print(f\"Example {i} prediction: {name} ({100*p}%)\")\n",
    "\n",
    "  # Save predictions to dataframe\n",
    "  df_predict[\"label\"].iloc[i] = name\n",
    "  df_predict['certainty'].iloc[i] = format(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
